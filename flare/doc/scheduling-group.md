# 调度组

为了改善flare框架在多核、多线程环境下的伸缩性，我们从底层[fiber调度](fiber-scheduling.md)开始引入分组的思想，并向上暴露给flare的其他模块、子系统（对象池、io等）。我们将分组的结果称之为*调度组*。

调度组的大小（pthread workers个数）可以通过参数`flare_scheduling_group_size`控制。这一参数过大可能导致共享数据竞争激烈，调度组大小过小可能导致工作负载不均衡。通常来说我们不建议手动修改这一参数，可以考虑通过`flare_fiber_scheduling_optimize_for=(compute-heavy|compute|neutral|io|io-heavy)`来由框架确定一个适合具体场景的值。

flare的各层次在可行的情况下，均优先考虑将资源按照调度组划分其内部共享数据，以减少共享数据的竞争并提高伸缩性：

- [IO](io.md)为每个调度组分配了自己的事件循环
- RPC客户端内部按照调度组维护连接池
- ...

我们的调度逻辑通常避免在调度组间发生fiber的偷取，因此通常每个fiber的工作集集中在同一个调度组内。

这意味着各个fiber可能需要竞争的共享数据通常也处在这一调度组。因此，不同调度组之间通常只会由于被偷取的fiber访问原调度组导致的的线程间同步（mutex等）开销，其余情况下不同调度组之间的干扰比较小，**明显的改善我们框架的整体伸缩性**。

在[NUMA](https://en.wikipedia.org/wiki/Non-uniform_memory_access)环境中，即便发生任务偷取，跨NUMA节点的调度组间偷取也会受到相对于同NUMA节点内不同调度组间的偷取更强的频率限制。这也有利于我们将各个fiber的工作集集中在某个NUMA节点内，对系统（除上述管理资源需要明确按照调度组划分的）其他模块、子系统尽可能透明的**避免访问远端内存，降低内存访问延迟**。

对于多路环境，按照调度组划分工作集还有利于我们**改善跨CPU节点的通信消息数量，提高伸缩性**。以原子变量为例（关于原子变量的讨论可以参考[性能指引](performance-guide.md)），我们这儿通过将原子变量的活动范围控制在同一个调度组内（可能的情况下，如对象引用计数），也意味着将至控制在了同一个物理CPU节点内，这样原子操作引入的缓存连贯性消息通常不会跨越物理CPU节点，因此控制了原子变量的成本提高性能。

---
[返回目录](README.md)
